# üß† Ideas & Arquitectura: SSD-First, RAM-Shadow

Este documento detalla la filosof√≠a, mecanismos y hoja de ruta para lograr que la RAM sea una "sombra" eficiente mientras el SSD asume la carga pesada.

## üéØ Objetivo Central
**Invertir la jerarqu√≠a de memoria tradicional.**
En lugar de "Cargar todo a RAM para que sea r√°pido", el modelo es "Dejar todo en SSD y mapear solo lo necesario".

> **Meta:** Lograr un throughput de lectura de texto cercano a la velocidad nativa del NVMe (3-7 GB/s) con un uso de RAM constante y predecible (O(1) o O(N_indices)), independiente del tama√±o total de los datos.

## 1Ô∏è‚É£ Principios de Cooperaci√≥n RAM-SSD

### La RAM como "Mapa", no como "Territorio"
- **Rol de la RAM**: Solo debe conocer la *existencia* y *ubicaci√≥n* de los datos.
  - Estructura: `Hash(ID) -> { Offset, Length }`.
  - Costo: ~16 bytes por entrada. 1 mill√≥n de textos = ~16 MB de RAM.
- **Rol del SSD**: Contiene el *cuerpo* de los datos.
  - Estructura: `[Header][Entry][Data][Entry][Data]...`
  - Inmutable: Una vez escrito, no se mueve. Esto permite que el SO cachee agresivamente sin problemas de coherencia.

### El Sistema Operativo es el Cache Manager
- No reinventar la rueda. Linux/Windows gestionan la memoria virtual mejor que nosotros.
- **Mecanismo**: `mmap` (Memory Mapped File).
  - Cuando accedemos a un byte, si no est√° en RAM, el CPU dispara un *Page Fault*.
  - El SO pausa el hilo (microsegundos), carga la p√°gina de 4KB desde el NVMe a la RAM, y reanuda.
  - Si la RAM se llena, el SO descarta las p√°ginas m√°s viejas (LRU nativo del kernel).
  - **Ventaja**: No necesitamos un Garbage Collector ni un Cache Manager complejo en userspace.

## 2Ô∏è‚É£ Estrategias de Optimizaci√≥n (C√≥mo compensar la latencia)

Aunque el NVMe es r√°pido, es m√°s lento que la RAM. Para compensar:

### A. Paralelismo Masivo (Rayon)
- Los NVMe modernos tienen m√∫ltiples colas de hardware (NVMe Queues).
- Leer 1 string es lento (latencia). Leer 10,000 strings en paralelo satura el ancho de banda.
- **Implementaci√≥n**: Usar `rayon` para disparar m√∫ltiples *page faults* en paralelo. El controlador del SSD reordenar√° las peticiones para m√°xima eficiencia.

### B. Prefetching Inteligente (Coop. Activa)
- **El problema**: El SO es reactivo (espera al Page Fault).
- **La soluci√≥n**: Ser proactivos.
  - `madvise(MADV_WILLNEED)`: Decirle al kernel "Voy a usar este rango de memoria pronto".
  - El kernel inicia la lectura DMA as√≠ncrona desde el NVMe a la RAM *antes* de que el c√≥digo llegue ah√≠.
  - **Resultado**: Cuando el CPU pide el dato, ya est√° en RAM. Latencia cercana a cero.

### C. Estructura de Datos "Friendly" para Hardware
- **Alineaci√≥n**: Asegurar que los datos comiencen en m√∫ltiplos de p√°gina (4KB) para textos muy grandes (opcional, pero √∫til para Zero-Copy real).
- **Contig√ºidad**: Textos relacionados deber√≠an escribirse juntos en el archivo `PUF` para aprovechar la localidad espacial.

## 3Ô∏è‚É£ Metas T√©cnicas para "Ganar" a la RAM tradicional

1.  **Tiempo de Inicio Instant√°neo**:
    - RAM tradicional: Tiene que leer y parsear todo el archivo al inicio (lento).
    - ADead-Parallel: Solo lee el √≠ndice (o incluso mapea el √≠ndice). Inicio en milisegundos.

2.  **Resiliencia a Crashes**:
    - Si el proceso muere, los datos ya est√°n en disco. No hay `fsync` de p√°nico necesario.

3.  **Escalabilidad Infinita**:
    - Puedes tener un dataset de 10 TB en una m√°quina con 16 GB de RAM.
    - El rendimiento se degrada suavemente (thrashing) en lugar de crashear por OOM (Out of Memory).

## 4Ô∏è‚É£ Inteligencia Cooperativa Avanzada (NUEVO)

Para llevar la cooperaci√≥n al siguiente nivel, implementaremos:

### üß† A. Predicci√≥n de Acceso (Heur√≠stica)
- Si el usuario accede a `ID_100`, es probable que acceda a `ID_101` (localidad temporal).
- El sistema puede disparar un *prefetch* especulativo de los vecinos en el archivo f√≠sico.

### üßä B. Hot/Cold Tiering (Optimizaci√≥n de Layout)
- **Problema**: Con el tiempo, los datos "calientes" (muy usados) quedan dispersos entre datos "fr√≠os" (viejos).
- **Soluci√≥n Inteligente**:
  - Un proceso background analiza estad√≠sticas de acceso.
  - Reescribe un nuevo archivo `.puf` colocando todos los datos "Hot" juntos al principio.
  - **Beneficio**: Maximiza el uso de cada p√°gina de 4KB en RAM (densidad de informaci√≥n).

### üì¶ C. Compresi√≥n H√≠brida
- Textos peque√±os (< 64 bytes): Guardar raw (la descompresi√≥n es m√°s cara que la lectura).
- Textos grandes (> 4KB): Comprimir con LZ4/Zstd.
  - NVMe lee menos bytes -> Menos presi√≥n en bus PCIe.
  - CPU descomprime r√°pido en L3 cache.

### ‚ö° D. Async I/O Profundo (io_uring / IOCP)
- Para cargas extremas, saltar el `mmap` y usar I/O as√≠ncrono directo (`O_DIRECT`) para llenar buffers de usuario, evitando la gesti√≥n de p√°ginas del SO si detectamos que el patr√≥n de acceso es completamente aleatorio y masivo.

---

### üìù Resumen Ejecutivo
La "ventaja injusta" de esta arquitectura es que **delega la complejidad al hardware y al kernel**. Mientras otros pelean gestionando buffers en heap, nosotros dejamos que el MMU (Memory Management Unit) y el controlador NVMe hagan lo que mejor saben hacer: mover bits rapid√≠simo.
